export const lesson30Quiz = [
  {
    id: 1,
    question: "A company wants to host their email service without managing physical servers. Which cloud service model is most appropriate?",
    options: [
      "IaaS (Infrastructure as a Service)",
      "SaaS (Software as a Service) - provides ready-to-use email application (like Office 365, Gmail) managed entirely by provider",
      "PaaS (Platform as a Service)",
      "Cloud doesn't support email"
    ],
    correct: 1,
    explanation: "Cloud service models: SaaS (Software as a Service): (1) Complete application delivered over internet, (2) Provider manages everything (infrastructure, platform, application, data backup), (3) User just uses the application, (4) Examples: Office 365, Gmail, Salesforce, Dropbox, Zoom, Slack, (5) Benefits: no installation, automatic updates, accessible anywhere, scalable, (6) Use case: email, CRM, collaboration tools, productivity suites. PaaS (Platform as a Service): (1) Development platform provided, (2) User builds/deploys applications, (3) Provider manages infrastructure, OS, runtime, (4) Examples: Azure App Service, Google App Engine, Heroku, AWS Elastic Beanstalk, (5) Use case: application development without managing servers. IaaS (Infrastructure as a Service): (1) Virtual machines, storage, networking provided, (2) User manages OS, applications, data, (3) Provider manages physical hardware, virtualization, (4) Examples: AWS EC2, Azure Virtual Machines, Google Compute Engine, (5) Use case: full control, custom configurations, lift-and-shift migrations. Comparison: (1) SaaS = least control, least management, fastest deployment, (2) PaaS = moderate control, focus on code not infrastructure, (3) IaaS = most control, most management responsibility. Email hosting decision: SaaS best choice - ready immediately, no server management, provider handles security/updates/backups, mailbox management included. IaaS would require setting up Exchange/mail server (complex). Responsibility model: SaaS provider manages ~90%, user manages data/user access. IaaS user manages ~70%, provider manages physical infrastructure only."
  },
  {
    id: 2,
    question: "What is the difference between a public cloud, private cloud, and hybrid cloud?",
    options: [
      "All cloud types are identical",
      "Public cloud: shared infrastructure (AWS, Azure); Private cloud: dedicated to one organization; Hybrid cloud: combination of both with integration",
      "Private clouds are always slower",
      "Cloud types don't matter"
    ],
    correct: 1,
    explanation: "Cloud deployment models: Public cloud: (1) Shared infrastructure owned by cloud provider, (2) Multi-tenant (many organizations share resources), (3) Pay-as-you-go pricing, (4) Examples: AWS, Microsoft Azure, Google Cloud Platform, (5) Benefits: low upfront cost, unlimited scalability, no maintenance, global availability, (6) Drawbacks: less control, data security concerns, internet dependency, compliance limitations. Private cloud: (1) Dedicated infrastructure for single organization, (2) Can be on-premises or hosted by third party, (3) Examples: VMware vSphere private cloud, OpenStack, Azure Stack (on-prem Azure), (4) Benefits: complete control, enhanced security, regulatory compliance easier, customization, (5) Drawbacks: high upfront cost, limited scalability, maintenance burden, requires expertise. Hybrid cloud: (1) Combination of public and private clouds, (2) Integrated (data/apps move between environments), (3) Examples: Azure Arc, AWS Outposts, Google Anthos, (4) Benefits: flexibility (sensitive data in private, burst to public for capacity), cost optimization, gradual migration, (5) Common architecture: critical apps in private cloud, dev/test in public cloud. Community cloud: (1) Shared by organizations with common needs (healthcare, government), (2) Costs shared among members, (3) Compliance/regulatory benefits. Use case examples: (1) Startup = public cloud (low initial cost), (2) Bank = private cloud (security/compliance), (3) Healthcare = hybrid (patient data private, analytics public), (4) E-commerce = hybrid (website public for scalability, customer DB private for security). Migration patterns: public → hybrid (bring some workloads back), private → hybrid (extend to public for DR/burst)."
  },
  {
    id: 3,
    question: "A business needs to ensure their cloud-hosted application can handle traffic spikes during seasonal sales. What cloud characteristic enables this?",
    options: [
      "Cloud applications cannot scale",
      "Rapid elasticity/scalability - cloud resources automatically increase during high demand and decrease when demand drops, paying only for what's used",
      "Manual server purchase is required",
      "Applications always run at maximum capacity"
    ],
    correct: 1,
    explanation: "Cloud characteristics (NIST definition): Rapid elasticity: (1) Resources scale up/down automatically based on demand, (2) Appear unlimited to user, (3) Automatic (auto-scaling) or manual scaling, (4) Pay only for resources used (no idle capacity costs), (5) Minutes to deploy additional resources vs weeks for physical hardware. Scaling types: Vertical scaling (scale up): (1) Increase size of existing resources (bigger VM - more CPU/RAM), (2) Limits: maximum size of single instance, (3) Downtime may be required (resize VM). Horizontal scaling (scale out): (1) Add more instances (more VMs running application), (2) Load balancer distributes traffic across instances, (3) Preferred method (nearly unlimited), (4) No downtime (add instances while running), (5) Application must support (stateless design). Auto-scaling configuration: (1) Define minimum/maximum instances (e.g., min 2, max 20), (2) Scaling triggers (CPU >70%, add instance; CPU <30%, remove instance), (3) Cooldown period (prevent thrashing), (4) Scheduled scaling (Black Friday, known peaks). Real-world example: E-commerce site: (1) Normal: 4 web servers handle traffic, (2) Black Friday: auto-scale to 40 web servers, (3) Next day: scale back to 4 servers, (4) Only pay for 40 servers during spike (not year-round). Other cloud characteristics: (1) On-demand self-service (provision without human interaction), (2) Broad network access (accessible from anywhere), (3) Resource pooling (multi-tenant, shared infrastructure), (4) Measured service (pay per use, metering). Benefits: (1) Cost savings (pay for peaks, not for unused capacity), (2) Performance (handle unexpected traffic), (3) Reliability (scale up if instance fails). Limitations: (1) Poorly designed apps may not scale horizontally, (2) Databases harder to scale than web servers, (3) Costs can spike unexpectedly (set spending limits)."
  },
  {
    id: 4,
    question: "What is the purpose of a virtual private cloud (VPC) in cloud computing?",
    options: [
      "VPCs are physical data centers",
      "Isolated virtual network within public cloud that provides private IP space, subnets, routing tables, and network gateways - logically isolated from other customers",
      "VPCs only provide storage",
      "Virtual networks are unnecessary"
    ],
    correct: 1,
    explanation: "Virtual Private Cloud (VPC) concepts: Definition: (1) Isolated section of cloud provider's network, (2) Private IP address space (10.0.0.0/16 example), (3) Logically separated from other customers, (4) Full control over network configuration, (5) AWS VPC, Azure Virtual Network, Google VPC. Components: Subnets: (1) Subdivide VPC IP space (10.0.1.0/24, 10.0.2.0/24), (2) Public subnets (internet access) vs private subnets (no direct internet), (3) Availability zone placement (redundancy). Route tables: (1) Control traffic routing within VPC and to internet, (2) Different routes for different subnets. Internet gateway: (1) Allows public subnet resources to access internet, (2) NAT gateway for private subnet outbound internet (without exposing resources). Security groups: (1) Virtual firewalls for instances, (2) Stateful (return traffic automatic), (3) Allow rules (deny implicit). Network ACLs: (1) Subnet-level firewalls, (2) Stateless (must allow both directions), (3) Allow and deny rules. VPN/Direct Connect: (1) Secure connection from on-premises to VPC, (2) Hybrid cloud connectivity. Common architecture: (1) Public subnet: web servers with public IPs, internet gateway, (2) Private subnet: database servers, no public IPs, NAT gateway for updates, (3) Security groups: web tier allows 80/443, DB tier allows 3306/1433 only from web tier. Benefits: (1) Network isolation (security), (2) Control over IP addressing, (3) Hybrid connectivity (extend on-prem network), (4) Compliance (network segmentation requirements). Multi-VPC strategy: (1) Separate VPCs for dev/test/prod, (2) Separate VPCs per application/department, (3) VPC peering (connect VPCs privately). Comparison to physical: VPC is like corporate network in cloud - private, controlled, but virtual infrastructure."
  },
  {
    id: 5,
    question: "A company stores sensitive customer data in the cloud. Who is responsible for encrypting that data?",
    options: [
      "Cloud provider is always responsible for everything",
      "Shared responsibility: provider secures infrastructure (physical security, hypervisor), customer secures data (encryption, access control, application security)",
      "Nobody is responsible for cloud security",
      "Only the customer is responsible for all security"
    ],
    correct: 1,
    explanation: "Cloud shared responsibility model: Cloud provider responsibilities (security OF the cloud): (1) Physical security (data center access, cameras, guards), (2) Hardware (servers, storage, networking), (3) Hypervisor/virtualization layer, (4) Network infrastructure (physical networks), (5) Facilities (power, cooling, fire suppression), (6) Compliance certifications (SOC 2, ISO 27001). Customer responsibilities (security IN the cloud): (1) Data encryption (at rest and in transit), (2) Access control (IAM, user permissions), (3) Application security (code vulnerabilities), (4) Operating system patching (IaaS), (5) Network configuration (security groups, ACLs), (6) Identity management (MFA, password policies), (7) Data classification and handling. Service model differences: SaaS: (1) Provider manages most (infrastructure, platform, application), (2) Customer manages: data, user access, identity, (3) Example: Office 365 - Microsoft secures application, customer controls who accesses files. PaaS: (1) Provider manages infrastructure and platform, (2) Customer manages: applications, data, access control, (3) Example: Azure App Service - Microsoft secures platform, customer secures app code. IaaS: (1) Provider manages infrastructure only, (2) Customer manages: OS, applications, data, network config, (3) Example: AWS EC2 - Amazon secures hypervisor, customer patches OS. Data encryption responsibility: (1) Customer must enable encryption (not default always), (2) Customer manages encryption keys, (3) Customer controls who can decrypt, (4) Provider offers tools (KMS, key vault) but customer configures. Common misconceptions: (1) 'Cloud provider handles all security' - FALSE, (2) 'Data in cloud is automatically encrypted' - FALSE (must enable), (3) 'Provider can access my data' - depends on encryption key management. Best practices: (1) Encrypt data before upload (client-side), (2) Use provider's key management (AWS KMS, Azure Key Vault), (3) Enable MFA for all accounts, (4) Regular access reviews, (5) Security monitoring and logging."
  },
  {
    id: 6,
    question: "What is the benefit of cloud file storage services like OneDrive, Google Drive, or Dropbox compared to local storage?",
    options: [
      "Cloud storage is always slower with no benefits",
      "Accessibility from any device/location, automatic backup/sync, collaboration features, and protection against local hardware failure",
      "Cloud storage is only for photos",
      "Local storage is superior in every way"
    ],
    correct: 1,
    explanation: "Cloud storage benefits: Accessibility: (1) Access files from any device (PC, phone, tablet), (2) Any location with internet, (3) No USB drive needed, (4) Web browser access (no software required), (5) Cross-platform (Windows, Mac, Linux, iOS, Android). Automatic sync: (1) Save on one device, available on all devices, (2) Real-time or near-real-time synchronization, (3) Selective sync (choose which folders sync to which devices), (4) Offline access (cached locally, sync when connected). Backup and recovery: (1) Protection against hardware failure (laptop dies, files safe), (2) Ransomware protection (version history, restore previous versions), (3) Accidental deletion recovery (recycle bin, 30+ day retention), (4) Disaster recovery (fire, theft, data preserved). Collaboration: (1) Share files/folders with others (links, permissions), (2) Real-time co-editing (multiple users editing simultaneously), (3) Comments and activity tracking, (4) Version history (see who changed what, when), (5) No emailing attachments back and forth. Storage capacity: (1) Scalable (purchase more as needed), (2) Often cheaper than buying external drives, (3) No physical storage management. Common services: OneDrive: (1) Microsoft, 5-100 GB+ free (with Office 365), (2) Office integration (edit docs in browser), (3) Windows integration (built-in). Google Drive: (1) Google, 15 GB free (shared with Gmail), (2) Google Docs/Sheets integration, (3) Search capabilities. Dropbox: (1) 2 GB free, (2) Excellent sync technology, (3) Business features. iCloud: (1) Apple ecosystem, 5 GB free, (2) Seamless iOS/Mac integration. Security considerations: (1) Encryption in transit (HTTPS) and at rest, (2) Two-factor authentication (enable!), (3) Shared link expiration, password protection, (4) Privacy concerns (provider can access unencrypted data). Use cases: (1) Personal: photos, documents, automatic phone backup, (2) Business: project collaboration, file sharing, mobile workforce. Limitations: (1) Requires internet (offline mode limited), (2) Subscription costs for large storage, (3) Bandwidth consumption (large file uploads/downloads), (4) Privacy/compliance concerns for sensitive data."
  },
  {
    id: 7,
    question: "What is the purpose of cloud load balancing?",
    options: [
      "Load balancing only affects weight distribution",
      "Distributes incoming traffic across multiple servers/instances to prevent overload, improve performance, and provide high availability/fault tolerance",
      "Load balancers slow down applications",
      "Cloud applications don't need load balancing"
    ],
    correct: 1,
    explanation: "Cloud load balancing: Purpose and benefits: (1) Distribute traffic across multiple backend servers (no single server overloaded), (2) High availability (if one server fails, others handle traffic), (3) Scalability (add more servers behind load balancer), (4) Performance (users routed to least-busy or closest server), (5) Maintenance (take servers offline without downtime). How it works: (1) User connects to load balancer (single public IP/DNS name), (2) Load balancer distributes connection to backend server based on algorithm, (3) Backend servers process request, respond through load balancer (or directly), (4) Health checks (load balancer monitors server health, removes failed servers from pool). Load balancing algorithms: Round robin: (1) Each server gets equal requests in rotation, (2) Simple, works well if servers equal capacity. Least connections: (1) New request to server with fewest active connections, (2) Better for variable request durations. Weighted: (1) Servers assigned weight (more powerful servers get more traffic), (2) Example: new server 50% capacity gets 50% traffic. IP hash: (1) Client IP determines server (same client = same server), (2) Session persistence without cookies. Geolocation: (1) Route based on user location (lowest latency). Types: Layer 4 (Transport): (1) Load balance based on IP/port, (2) Faster (doesn't inspect content), (3) Protocol agnostic (TCP/UDP). Layer 7 (Application): (1) Load balance based on HTTP headers, URLs, cookies, (2) Advanced routing (send /api to API servers, /images to CDN), (3) SSL termination, (4) Content-based routing. Health checks: (1) Periodic requests to backend servers (HTTP 200 OK = healthy), (2) Failed health checks = remove server from pool, (3) Automatic return when healthy again. Session persistence (sticky sessions): (1) Same user routes to same server (session data preserved), (2) Cookie-based or IP-based, (3) Reduces session management complexity. Cloud provider solutions: (1) AWS Elastic Load Balancer (ALB, NLB), (2) Azure Load Balancer, (3) Google Cloud Load Balancing. Example: (1) E-commerce site with 10 web servers behind load balancer, (2) User connects to loadbalancer.example.com, (3) Load balancer sends to webserver05, (4) Webserver05 crashes, health check detects, removes from pool, (5) Subsequent requests go to remaining 9 servers."
  },
  {
    id: 8,
    question: "What is Infrastructure as Code (IaC) in cloud computing?",
    options: [
      "Writing code inside infrastructure",
      "Managing infrastructure through code/scripts instead of manual configuration - enables automated, repeatable, version-controlled deployments",
      "IaC only works for developers",
      "Infrastructure cannot be defined as code"
    ],
    correct: 1,
    explanation: "Infrastructure as Code (IaC): Definition: (1) Define and manage infrastructure using code files instead of manual processes, (2) Declarative (what you want) or imperative (how to do it), (3) Version controlled (Git), (4) Automated deployment, (5) Treat infrastructure like application code. Benefits: Repeatability: (1) Deploy identical environments every time (dev, test, prod identical), (2) No configuration drift (manual changes accumulating), (3) Disaster recovery (recreate infrastructure from code in minutes). Version control: (1) Infrastructure changes tracked in Git, (2) Review changes before applying (pull requests), (3) Rollback if deployment fails (revert to previous version), (4) Audit trail (who changed what, when). Speed: (1) Deploy complex infrastructure in minutes (not hours/days of manual work), (2) Automated (no clicking through web console), (3) Self-service (developers provision without waiting for ops team). Consistency: (1) No human error (forgot to configure firewall rule), (2) Best practices codified, (3) Compliance as code (security rules enforced). IaC tools: Terraform: (1) Multi-cloud (AWS, Azure, GCP, etc.), (2) Declarative (HCL language), (3) Plan before apply (preview changes), (4) State management. AWS CloudFormation: (1) AWS only, (2) JSON/YAML templates, (3) Native AWS integration. Azure Resource Manager (ARM) templates: (1) Azure only, (2) JSON templates. Ansible: (1) Configuration management and deployment, (2) Agentless (SSH-based), (3) YAML playbooks. Example workflow: (1) Write Terraform code defining VPC, subnets, EC2 instances, load balancer, (2) Store in Git repository, (3) terraform plan (preview changes), (4) terraform apply (create infrastructure), (5) Infrastructure deployed in 5 minutes (vs hours manual). Use cases: (1) Multi-environment deployment (dev/test/prod from same code), (2) Disaster recovery (recreate infrastructure quickly), (3) Scaling (deploy 100 servers with one command), (4) Compliance (enforce security standards in code). Challenges: (1) Learning curve (new skills), (2) State management (tracking current infrastructure), (3) Initial investment (writing code takes time upfront)."
  },
  {
    id: 9,
    question: "What is a content delivery network (CDN), and how does it improve performance?",
    options: [
      "CDN is a type of cable",
      "Distributed network of servers caching content closer to users globally - reduces latency by serving content from nearest location instead of origin server",
      "CDNs only store viruses",
      "Content delivery has no impact on performance"
    ],
    correct: 1,
    explanation: "Content Delivery Network (CDN): How CDN works: (1) Origin server hosts original content (website, videos, images), (2) CDN edge servers (Points of Presence) worldwide cache copies, (3) User requests content → DNS routes to nearest edge server, (4) Edge server serves cached content (fast, local), (5) Cache miss = edge server fetches from origin, caches, serves to user. Benefits: Reduced latency: (1) Content served from geographically close server (10ms vs 200ms), (2) Faster page load times, (3) Better user experience. Reduced origin load: (1) Most requests served by edge servers (not origin), (2) Origin handles only cache misses, (3) Scales better (origin doesn't get overwhelmed). Bandwidth savings: (1) Content delivered from edge (cheaper than origin bandwidth), (2) Reduced costs for origin hosting. Reliability: (1) Redundant edge servers (if one fails, another serves), (2) DDoS protection (attack absorbed by distributed network), (3) High availability. CDN providers: (1) Cloudflare, (2) Amazon CloudFront, (3) Azure CDN, (4) Akamai, (5) Fastly. What content CDN serves: Static content: (1) Images, CSS, JavaScript, (2) Videos, downloads, (3) Perfect for CDN (doesn't change frequently). Dynamic content: (1) Personalized pages, API responses, (2) Some CDNs cache with smart rules, (3) Edge computing (run code at edge). CDN features: Cache control: (1) Time-to-live (TTL) - how long to cache (1 hour, 1 day, 1 week), (2) Purge/invalidate cache (remove old content), (3) Cache headers (control what/how long to cache). SSL/TLS: (1) HTTPS termination at edge, (2) Secure delivery. Geographic restrictions: (1) Block/allow specific countries, (2) Compliance (GDPR, content licensing). Real-world example: (1) User in Tokyo visits US-based website, (2) Without CDN: request crosses Pacific (200ms latency), (3) With CDN: served from Tokyo edge server (10ms latency), (4) Page loads 20× faster. Use cases: (1) Global websites (faster for international users), (2) Video streaming (Netflix, YouTube use massive CDNs), (3) Software downloads (reduce origin costs), (4) E-commerce (faster product images = more sales), (5) Gaming (game updates distributed via CDN). Costs: (1) Pay per GB transferred, (2) Usually cheaper than origin bandwidth, (3) Free tiers available (Cloudflare)."
  },
  {
    id: 10,
    question: "What is serverless computing, and how does it differ from traditional cloud VMs?",
    options: [
      "Serverless means no servers exist",
      "Serverless runs code in response to events without managing servers - provider handles infrastructure, you only pay for actual execution time (not idle VMs)",
      "Serverless is slower than VMs",
      "VMs and serverless are identical"
    ],
    correct: 1,
    explanation: "Serverless computing: Definition: (1) Run code without provisioning/managing servers, (2) Provider automatically handles infrastructure, scaling, patching, (3) Pay only for execution time (not idle time), (4) Event-driven execution, (5) Examples: AWS Lambda, Azure Functions, Google Cloud Functions. How it works: (1) Write function code (JavaScript, Python, etc.), (2) Upload to serverless platform, (3) Configure trigger (HTTP request, file upload, schedule, database change), (4) Function executes when triggered, (5) Scales automatically (1 to 1000s of concurrent executions), (6) Terminates after execution (no idle time). Serverless vs traditional VMs: Infrastructure management: (1) Serverless: zero (provider manages everything), (2) VMs: customer manages OS, patching, scaling. Scaling: (1) Serverless: automatic, instant (0 to millions of requests), (2) VMs: manual or auto-scaling (configure rules, takes minutes). Cost: (1) Serverless: pay per millisecond of execution time + requests, (2) VMs: pay per hour even if idle (always running). Use cases: (1) Serverless: event processing, APIs, scheduled tasks, image processing, (2) VMs: long-running applications, stateful applications, specific OS requirements. Serverless characteristics: Stateless: (1) Each function execution independent, (2) No local persistent storage, (3) Use external storage (databases, object storage). Short-lived: (1) Execution time limits (AWS Lambda 15 minutes max), (2) Not for long-running processes. Cold starts: (1) First execution after idle period slower (loading code), (2) Subsequent executions fast (warm start). Advantages: (1) No server management (focus on code, not infrastructure), (2) Automatic scaling (handle any load), (3) Cost-efficient (pay only for use, not idle time), (4) Fast deployment (upload code, done). Disadvantages: (1) Vendor lock-in (code tied to specific platform), (2) Cold start latency, (3) Debugging harder (no direct server access), (4) Limited execution time (not for long processes), (5) Stateless constraint (architectural consideration). Real-world examples: (1) Image thumbnail generation: upload photo → Lambda generates thumbnail → stores in S3, (2) API backend: mobile app → API Gateway → Lambda → DynamoDB, (3) Scheduled reports: CloudWatch schedule → Lambda generates report → emails. FaaS (Functions as a Service): serverless is subset of FaaS - function-level granularity."
  },
  {
    id: 11,
    question: "A company needs to connect their on-premises data center to their cloud VPC with a dedicated, high-bandwidth connection. What solution should they use?",
    options: [
      "Regular internet connection only",
      "Dedicated connection service like AWS Direct Connect or Azure ExpressRoute - private, high-speed link bypassing public internet",
      "VPCs cannot connect to on-premises",
      "Email is the only connection method"
    ],
    correct: 1,
    explanation: "Cloud connectivity options: VPN over internet: (1) Encrypted tunnel over public internet, (2) Uses site-to-site VPN (on-prem router to cloud VPN gateway), (3) Cheap (uses existing internet), (4) Variable performance (internet dependent), (5) 10-100 Mbps typical, (6) Good for: small data transfers, non-critical workloads, low budget. Dedicated connections: AWS Direct Connect: (1) Dedicated network connection from on-prem to AWS, (2) 1 Gbps or 10 Gbps circuits, (3) Private (doesn't traverse internet), (4) Consistent performance (low latency, high bandwidth). Azure ExpressRoute: (1) Microsoft's dedicated connection service, (2) 50 Mbps to 10 Gbps options, (3) Private peering (connect to Azure VMs), (4) Microsoft peering (connect to Office 365, Dynamics 365). Google Cloud Interconnect: (1) Dedicated or partner interconnect, (2) 10 Gbps or 100 Gbps links. Benefits of dedicated connections: (1) Higher bandwidth (1-100 Gbps vs 100 Mbps VPN), (2) Lower latency (private link, no internet congestion), (3) More consistent (predictable performance), (4) More secure (private, not over public internet), (5) Reduced data transfer costs (cheaper than internet transfer in many cases). Setup process: (1) Order circuit from network provider (months lead time), (2) Physical cross-connect in colocation facility, (3) Configure BGP routing, (4) Create virtual interfaces to VPCs, (5) Route traffic over dedicated link. Costs: (1) Port hour charges ($0.30-$2.00 per hour depending on speed), (2) Data transfer charges (egress), (3) Often cheaper than internet transfer for high volumes, (4) Requires upfront commitment (not pay-as-you-go). Redundancy: (1) Best practice: dual connections (two Direct Connect/ExpressRoute), (2) Active-active or active-passive, (3) VPN as backup (if primary fails). Hybrid architecture: (1) Critical data over Direct Connect (fast, private), (2) VPN for backup/secondary workloads, (3) Hybrid cloud connectivity foundation. Use cases: (1) Large data transfers (TB to cloud), (2) Hybrid cloud applications, (3) Disaster recovery replication, (4) Latency-sensitive applications, (5) Regulatory compliance (data must stay on private network)."
  },
  {
    id: 12,
    question: "What is cloud bursting, and when would it be used?",
    options: [
      "Cloud bursting destroys clouds",
      "Automatically expanding from on-premises to public cloud during peak demand, then contracting back when demand normalizes - handles traffic spikes cost-effectively",
      "Cloud bursting only works in storms",
      "Applications cannot burst to cloud"
    ],
    correct: 1,
    explanation: "Cloud bursting strategy: Definition: (1) Hybrid cloud architecture, (2) Primary workload runs on-premises or private cloud, (3) Overflow traffic 'bursts' to public cloud during peaks, (4) Returns to on-premises when demand normalizes, (5) Elastic capacity without permanent public cloud costs. How it works: (1) Application runs in on-prem data center (baseline capacity), (2) Load monitor detects high demand (CPU, memory, requests), (3) Automatically provisions resources in public cloud, (4) Load balancer routes overflow traffic to cloud instances, (5) Demand decreases → cloud instances terminated, (6) Returns to baseline (on-prem only). Use cases: Seasonal peaks: (1) Retail: Black Friday, holiday shopping (massive traffic spike), (2) Tax preparation: January-April (then quiet), (3) Education: registration periods (enrollment). Unpredictable spikes: (1) Viral marketing campaign, (2) Breaking news (news sites), (3) Special events. Development/testing: (1) Baseline production on-prem, (2) Burst to cloud for testing (large-scale load tests). Benefits: (1) Cost savings: pay for cloud only during peaks (not year-round excess capacity), (2) Avoid upfront CAPEX: don't buy hardware for rare peaks, (3) Maintain control: sensitive data stays on-prem, (4) Flexibility: scale beyond physical capacity limits. Requirements: (1) Hybrid connectivity (VPN or Direct Connect), (2) Application portability (can run on-prem and cloud), (3) Automated orchestration (Kubernetes, cloud APIs), (4) Load balancing across environments, (5) Data synchronization (shared storage or replication). Challenges: (1) Application compatibility (must work in both environments), (2) Data transfer latency (if data on-prem but compute in cloud), (3) Complexity (managing hybrid environment), (4) Costs can spiral if not monitored (unexpected cloud spend). Example: E-commerce site: (1) Normal days: 10 on-prem servers handle traffic, (2) Black Friday: demand 10×, (3) Burst to cloud: 90 additional cloud instances, (4) Dec 1: terminate cloud instances, back to 10 on-prem, (5) Saved money vs buying 100 on-prem servers for one day. Technologies: (1) VMware Cloud on AWS (extend vSphere to AWS), (2) Azure Stack (Azure on-prem), (3) Google Anthos (multi-cloud), (4) Kubernetes with cluster federation. Alternative: permanent public cloud with auto-scaling (no on-prem) simpler but higher ongoing costs."
  },
  {
    id: 13,
    question: "What is the purpose of cloud backup and disaster recovery services?",
    options: [
      "Backups are unnecessary in the cloud",
      "Automated backup of data to cloud storage and ability to quickly restore/failover in case of data loss, corruption, or disaster - protection against hardware failure, ransomware, disasters",
      "Cloud data never needs backup",
      "Disaster recovery only works for weather"
    ],
    correct: 1,
    explanation: "Cloud backup and disaster recovery: Cloud backup purpose: (1) Protect data against loss (deletion, corruption, ransomware), (2) Offsite storage (safe from local disasters - fire, flood), (3) Automated (scheduled, no manual intervention), (4) Versioning (restore from multiple points in time), (5) Long-term retention (regulatory compliance). Backup types: File-level: (1) Back up specific files/folders (documents, photos), (2) Services: Backblaze, Carbonite, CrashPlan, (3) Continuous or scheduled backups. Image-based: (1) Complete system image (OS, apps, data), (2) Bare-metal recovery (restore entire system to new hardware), (3) Services: Veeam, Acronis, Azure Backup. Database backups: (1) Application-consistent database snapshots, (2) Point-in-time recovery, (3) Services: AWS RDS automated backups, Azure SQL Database. Disaster recovery (DR): Definition: (1) Ability to recover IT infrastructure after disaster, (2) Resume business operations quickly, (3) Minimize downtime (RTO - Recovery Time Objective), (4) Minimize data loss (RPO - Recovery Point Objective). DR strategies: Backup and restore (lowest cost): (1) Regular backups to cloud, (2) Restore when needed, (3) RTO: hours to days, RPO: hours (last backup), (4) Good for: non-critical systems. Pilot light: (1) Minimal environment always running in cloud (core services), (2) Scale up during disaster, (3) RTO: hours, RPO: minutes, (4) Good for: moderate criticality. Warm standby: (1) Scaled-down replica running in cloud, (2) Increase capacity during disaster, (3) RTO: minutes, RPO: seconds, (4) Good for: important systems. Hot standby/multi-site: (1) Full environment running in parallel, (2) Active-active or active-passive, (3) RTO: seconds, RPO: near-zero, (4) Good for: mission-critical (costly). Cloud DR benefits: (1) No secondary data center needed (cloud is DR site), (2) Pay for resources only during DR event (warm/hot standby costs lower than physical), (3) Geographic redundancy (cloud regions worldwide), (4) Fast recovery (automate failover). Services: (1) AWS Backup (centralized backup management), (2) Azure Site Recovery (DR orchestration), (3) Veeam Backup & Replication (on-prem and cloud). Best practices: (1) 3-2-1 rule: 3 copies, 2 different media types, 1 offsite (cloud), (2) Test DR regularly (verify restores work), (3) Document procedures (runbook for DR event), (4) Immutable backups (protection against ransomware deletion). Compliance: many regulations require offsite backups and DR testing (HIPAA, SOX, PCI-DSS)."
  },
  {
    id: 14,
    question: "What is multi-tenancy in cloud computing, and why does it matter?",
    options: [
      "Multi-tenancy means multiple buildings",
      "Multiple customers (tenants) share same physical infrastructure while logically isolated - enables cost efficiency and resource pooling, but requires strong isolation for security",
      "Each customer gets dedicated hardware",
      "Tenants only refer to apartment renters"
    ],
    correct: 1,
    explanation: "Multi-tenancy concepts: Definition: (1) Multiple organizations (tenants) share same physical infrastructure (servers, storage, network), (2) Logically isolated (tenant A cannot access tenant B's data), (3) Appears dedicated to each tenant (own environment), (4) Foundation of public cloud economics. How it works: (1) Hypervisor isolates VMs from different tenants, (2) Network segmentation (VPCs, VLANs), (3) Storage isolation (encryption, access controls), (4) Database-level isolation (separate schemas or databases), (5) Resource allocation (limits prevent one tenant hogging resources). Benefits: Cost efficiency: (1) Shared infrastructure = lower per-tenant cost, (2) Economies of scale (provider buys hardware in bulk), (3) Higher utilization (servers not idle), (4) Pay only for consumption (not entire server). Resource pooling: (1) Combined resources available to all tenants, (2) Oversubscription (not all tenants peak simultaneously), (3) Better utilization (someone always needs resources). Maintenance: (1) Provider updates infrastructure centrally, (2) All tenants benefit from improvements, (3) Reduced per-tenant maintenance burden. Security concerns: Isolation critical: (1) Hypervisor vulnerabilities could allow VM escape (tenant accessing another), (2) Side-channel attacks (Spectre, Meltdown - read other tenant's memory), (3) Network bleed (misconfigured isolation = data leak). Provider responsibilities: (1) Strong hypervisor security, (2) CPU vulnerability patches, (3) Network segmentation, (4) Encryption (data at rest, in transit), (5) Compliance certifications (SOC 2, ISO 27001). Noisy neighbor: (1) One tenant consuming excessive resources, (2) Impacts other tenants' performance, (3) Provider must enforce limits (CPU, IOPS, network). Single-tenancy (alternative): (1) Dedicated infrastructure for one customer, (2) Dedicated Hosts (AWS), Isolated VMs (Azure), (3) More expensive (no sharing), (4) Better for: compliance, security requirements, performance consistency. Multi-tenancy levels: Infrastructure: (1) Multiple VMs on same physical host (IaaS). Application: (1) Multiple customers using same application instance (SaaS - Salesforce, Office 365), (2) Shared database with tenant ID columns, or separate databases. Hybrid: (1) Shared infrastructure, isolated data (common). Compliance considerations: (1) Some regulations prohibit multi-tenancy (strict data isolation), (2) Government/defense often require single-tenancy, (3) Healthcare (HIPAA) usually OK with multi-tenancy if proper controls, (4) Financial (PCI-DSS) accepted with proper isolation. Example: AWS EC2 - multiple customers' VMs on same physical server, isolated by hypervisor."
  },
  {
    id: 15,
    question: "What is the purpose of cloud metering and monitoring?",
    options: [
      "Metering only measures electricity",
      "Tracking resource usage for billing (pay-per-use) and monitoring performance/health of cloud resources - enables cost optimization and operational visibility",
      "Cloud resources don't need monitoring",
      "Monitoring is only for weather"
    ],
    correct: 1,
    explanation: "Cloud metering and monitoring: Metering (billing): Purpose: (1) Track exact resource consumption (compute hours, storage GB, data transfer), (2) Generate accurate bills (pay only for what's used), (3) Enable chargeback/showback (allocate costs to departments/projects). What's metered: (1) Compute: VM hours by instance type (t2.micro, m5.large), (2) Storage: GB-months (100 GB for 1 month), (3) Data transfer: GB out (egress), (4) Requests: API calls, Lambda invocations, (5) Database: capacity, IOPS, backup storage. Billing models: (1) Pay-as-you-go (on-demand, per hour/second), (2) Reserved (commit 1-3 years, 30-70% discount), (3) Spot/preemptible (bid on unused capacity, up to 90% discount but can be terminated). Cost monitoring: (1) AWS Cost Explorer, Azure Cost Management, Google Cloud Billing, (2) Budgets and alerts (notify if spending exceeds threshold), (3) Cost anomaly detection (unusual spend spikes), (4) Recommendations (rightsizing, unused resources). Monitoring (operations): Purpose: (1) Ensure resources healthy and performing, (2) Detect issues proactively (before users complain), (3) Troubleshooting (identify root cause), (4) Capacity planning (trend analysis). What's monitored: (1) Metrics: CPU, memory, disk, network utilization, (2) Availability: uptime, health checks, (3) Performance: latency, throughput, error rates, (4) Logs: application logs, system logs, access logs, (5) Distributed tracing: request flow through microservices. Monitoring tools: (1) AWS CloudWatch (metrics, logs, alarms), (2) Azure Monitor (metrics, logs, Application Insights), (3) Google Cloud Monitoring (formerly Stackdriver), (4) Third-party: Datadog, New Relic, Prometheus. Alarms and alerts: (1) Threshold-based (CPU >80% for 5 minutes), (2) Anomaly detection (ML-based), (3) Notification channels (email, SMS, Slack, PagerDuty), (4) Auto-remediation (alarm triggers Lambda to restart instance). Dashboards: (1) Visual representation (graphs, charts), (2) Real-time and historical views, (3) Custom dashboards per team/application, (4) Executive dashboards (high-level KPIs). Logging: (1) Centralized log aggregation (all services to one place), (2) Search and analysis (find errors, trace requests), (3) Retention (compliance, troubleshooting), (4) Log insights (query language for analysis). Best practices: (1) Set up monitoring before issues occur, (2) Define KPIs and SLAs, (3) Alarm on symptoms, not just thresholds (page load time, not just CPU), (4) Regular review (adjust thresholds, retire unused alarms), (5) Cost optimization (identify idle resources, rightsizing opportunities). Example use: (1) Website slow → check CloudWatch → database CPU at 100% → alarm → scale up database → resolved in 5 minutes."
  }
];